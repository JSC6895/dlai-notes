# 2.2 Logistic 回归

> 视频：<https://mooc.study.163.com/learn/deeplearning_ai-2001281002?tid=2001392029#/learn/content?type=detail&id=2001701008>

本视频中，我们讲讲logistic回归，这是一个学习算法，用在监督学习问题中，输出`y`标签是`0`或`1`时，这是一个二元分类问题。已知的输入特征向量`x`可能是一张图，你希望识别出这是不是猫图。你需要一个算法，可以给出一个预测值，我们说它是预测值`y_hat`，就是你对`y`的预测。

更正式的说 你希望`y_hat`是一个概率，当输入特征`x`满足条件时，`y`就是`1`。所以换句话说，如果`x`是图片，正如我们在上一个视频中看到的，你希望`y_hat`能告诉你，这是一张猫图的概率。所以`x`正如我们之前的视频里说过的，是一个`n_x`维向量。

![](img/2-2-1.jpg)

已知 Logistic 回归的参数是`w`，也是一个`n_x`维向量，而`b`就是一个实数。所以已知输入`x`和参数`w`和`b`，我们如何计算输出`y_hat`？好，你可以这么试，但其实不靠谱，就是`y_hat = w^T·x + b`，输入`x`的线性函数。事实上，如果你做线性回归，就是这么算的，但这不是一个非常好的二元分类算法。因为你希望`y_hat`是`y=1`的概率，所以`y_hat`应该介于`0`和`1`之间。但实际上这很难实现，因为`w^Tx+b`可能比`1`大得多，或者甚至是负值，这样的概率是没意义的。

![](img/2-2-2.jpg)

你希望概率介于`0`和`1`之间，所以在 Logisitc 回归中，我们的输出变成`y_hat`等于 sigmoid 函数，作用到这个量上。这就是 sigmoid 函数的图形。横轴是`z`，那么`sigmoid(z)`就是这样的，从`0`到`1`的光滑函数。我标记一下这里的轴，这是`0`然后和垂直轴相交在`0.5`处。这就是`sigmoid(z)`的图形，我们用`z`来表示这个量，`w^Tx+b`。

![](img/2-2-3.jpg)

这是 Sigmoid 函数的公式，`sigmoid(z)`就是`1/(1+e^-z)`，其中`z`是实数。要注意一些事情，如果`z`非常大 那么`e^-z`就很接近`0`。那么`sigmoid(z)`就是，大约等于`1/(1+`某个很接近`0`的量。因为`e^-z`在`z`很大时就很接近`0`，所以这接近`1`。事实上，如果你看看左边的图，`z`很大时`sigmoid(z)`就很接近`1`。相反如果`z`很小，或者是非常大的负数，那么`sigmoid(z)`，`1/(1+e^-z)`，就变成很大的数字。这就变成...想一下，`1`除以`1`加上很大的数字，那是非常大的数字，所以接近`0`。确实，当你看到`z`变成非常大的负值时，`sigmoid(z)`就很接近`0`。

![](img/2-2-4.jpg)

所以当你实现 logistic 回归时，你要做的是学习参数`w`和`b`，所以`y_hat`变成了对`y=1`概率的比较好的估计。

在继续之前，我们再讲讲符号约定。当我们对神经网络编程时，我们通常会把`w`和参数`b`分开，这里`b`对应一个拦截器。在其他一些课程中，你们可能看过不同的表示。在一些符号约定中，你定义一个额外的特征，叫`x_0`，它等于`1`。所以`x`就是`R^(n_x+1)`维向量，然后你将`y_hat`定义为`σ(θ^Tx)`。

![](img/2-2-5.jpg)

在这另一种符号约定中，你有一个向量参数`θ`，等于`θ_0`、`θ_1`还有`θ_2`，一直到`θ_nx`。所以`θ_0`扮演的是`b`的角色，这是一个实数。而`θ_1`直到`θ_nx`的作用和`w`一样。事实上，当你实现你的神经网络时，将`b`和`w`看作独立的参数可能更好。所以对于这门课，我们不会用那种符号约定，就是红色写的那些，我不会用。如果你们没有在其他课程里见过这个符号约定，不要担心太多。我讲这个是为了服务那些，见过这种符号约定的学生。我们在本课中不会使用那种符号约定。如果你以前没见过，这不重要，所以不用担心。

现在你看到了 logistic 回归模型长什么样，接下来我们看参数`w`和`b`，你需要定义一个成本函数，我们下一个视频中讨论。
