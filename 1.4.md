# 1.4 深层 logistic 神经网络

> 视频：[第四周 深层神经网络](https://mooc.study.163.com/learn/deeplearning_ai-2001281002?tid=2001392029)

> 整理：[飞龙](https://github.com/wizardforcel)

对于上一节那个模型，如果我们打算调节隐层的节点数量，我们可以调节`n_hidden_nodes`的大小。但是如果我们打算调节隐层的数量，就不能将其硬编码。

我们可以从它里面总结一些规律。这里，我们使用 $n^{[l]}$ 表示第 $l$ 层的节点数。并将输入层看做第零层，也就是 $X$ 看做 $A^{[0]}$。

$$
Z^{[l]} = Z^{[l-1]} \Theta^{[l]} \\\\
A^{[l]} = \sigma(Z^{[l]}) \\\\
\frac{dJ}{dZ^{[l]}} = \frac{dJ}{dA^{[l]}} \ast A^{[l]} \ast(1-A^{[l]}) \\\\
\frac{dJ}{d\Theta^{[l]}} = Z^{[l-1]T} \frac{dJ}{dZ^{[l]}} \\\\
\frac{dJ}{dZ^{[l-1]}} = \frac{dJ}{dZ^{[l]}} \Theta^{[l]T}
$$

非常简单，只需要把上标`1`替换为`l`，`0`替换为`l-1`。

对于输出层，假设上标为`m`：

$$
J = -Sum(Y \ast \log(A^{[m]}) + (1-Y) \ast \log(1-A^{[m]})) \\\\
\frac{dJ}{dA^{[m]}} = \frac{1-Y}{1-A^{[m]}} - \frac{Y}{A^{[m]}}
$$

这里的除是逐元素除。

各个量的维度如下：

| 量 | 维度 |
| --- | --- |
| $\Theta^{[l]}$ | $n^{[l-1]} \times n^{[l]}$ |
| $Z^{[l]}$ $A^{[l]}$ | $n_data \times n^{[l]}$ |

## 代码

（待补充）
